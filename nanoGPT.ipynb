{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:29:26.446048Z",
     "start_time": "2024-07-26T16:29:20.785016Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import inspect\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "from dataclasses import dataclass\n",
    "import math\n",
    "\n",
    "# Custom Dataset class"
   ],
   "id": "80c8af43d86650c5",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:29:26.458328Z",
     "start_time": "2024-07-26T16:29:26.446048Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from torch.utils.data import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class PersianPoetryDataset(Dataset):\n",
    "    def __init__(self, text, block_size, tokenizer):\n",
    "        self.block_size = block_size\n",
    "        self.tokenizer = tokenizer\n",
    "        lines = text.split('\\n')\n",
    "        formatted_lines = []\n",
    "        for i in range(0, len(lines), 2):\n",
    "            if i + 1 < len(lines):\n",
    "                formatted_lines.append(f\"[BOM] {lines[i].strip()} [EOS] [BOM] {lines[i+1].strip()} [EOS]\")\n",
    "        text_with_bom_eos = ' '.join(formatted_lines)\n",
    "        self.tokens = tokenizer(text_with_bom_eos, return_tensors='pt')['input_ids'].squeeze()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.block_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        chunk = self.tokens[idx:idx + self.block_size + 1]\n",
    "        return chunk[:-1], chunk[1:]\n",
    "\n",
    "\n"
   ],
   "id": "d422a5bcbdcb242f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:29:26.497470Z",
     "start_time": "2024-07-26T16:29:26.458328Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int\n",
    "    vocab_size: int\n",
    "    n_layer: int\n",
    "    n_head: int\n",
    "    n_embd: int\n",
    "    dropout: float\n",
    "    bias: bool\n",
    "\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\"LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False\"\"\"\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
    "        self.c_proj = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size)).view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size()\n",
    "        q, k, v = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
    "        if self.flash:\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:, :, :T, :T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.c_proj = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        self.transformer.wte.weight = self.lm_head.weight  # Weight tying\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02 / math.sqrt(2 * config.n_layer))\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params() / 1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device)\n",
    "\n",
    "        tok_emb = self.transformer.wte(idx)\n",
    "        pos_emb = self.transformer.wpe(pos)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, [-1], :])\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters() if p.requires_grad}\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer"
   ],
   "id": "a18b10b1df4f2724",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:29:28.290960Z",
     "start_time": "2024-07-26T16:29:28.282994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "config = GPTConfig(\n",
    "    block_size=128,\n",
    "    vocab_size=25000,\n",
    "    n_layer=6,\n",
    "    n_head=4,\n",
    "    n_embd=128,\n",
    "    dropout=0.1,\n",
    "    bias=True\n",
    ")"
   ],
   "id": "f373bc7cb8079953",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T16:29:28.764022Z",
     "start_time": "2024-07-26T16:29:28.579897Z"
    }
   },
   "cell_type": "code",
   "source": "model = GPT(config)",
   "id": "91fb53ab9b665a3e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 4.39M\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T17:40:08.529734Z",
     "start_time": "2024-07-26T16:50:46.510248Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "with open('vahshi_norm.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.readlines()[:10000] \n",
    "    text = ''.join(text)\n",
    "\n",
    "# Create the dataset\n",
    "tokenizer = AutoTokenizer.from_pretrained('bolbolzaban/gpt2-persian')\n",
    "dataset = PersianPoetryDataset(text, config.block_size, tokenizer)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def train(model, dataset, epochs, batch_size, lr, weight_decay, device):\n",
    "    model = model.to(device)\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    optimizer = model.configure_optimizers(weight_decay, lr, (0.9, 0.95), device.type)\n",
    "    scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs)\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for i, (x, y) in enumerate(train_loader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits, loss = model(x, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            if i % 1000 == 0:\n",
    "                print(f'Epoch: {epoch}, Iteration: {i}, Loss: {loss.item()}')\n",
    "\n",
    "train(model, dataset, epochs=10, batch_size=8, lr=1e-5, weight_decay=0.1, device=device)\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), 'my_model1.pt')\n"
   ],
   "id": "17cfc2739b90f6df",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num decayed parameter tensors: 26, with 4,396,032 parameters\n",
      "num non-decayed parameter tensors: 50, with 10,240 parameters\n",
      "using fused AdamW: True\n",
      "Epoch: 0, Iteration: 0, Loss: 5.862204551696777\n",
      "Epoch: 0, Iteration: 1000, Loss: 5.203840255737305\n",
      "Epoch: 0, Iteration: 2000, Loss: 6.309654712677002\n",
      "Epoch: 0, Iteration: 3000, Loss: 5.6349992752075195\n",
      "Epoch: 0, Iteration: 4000, Loss: 5.537422180175781\n",
      "Epoch: 0, Iteration: 5000, Loss: 5.710529804229736\n",
      "Epoch: 0, Iteration: 6000, Loss: 5.983253479003906\n",
      "Epoch: 0, Iteration: 7000, Loss: 5.9268364906311035\n",
      "Epoch: 0, Iteration: 8000, Loss: 5.233710765838623\n",
      "Epoch: 0, Iteration: 9000, Loss: 5.796087265014648\n",
      "Epoch: 0, Iteration: 10000, Loss: 5.601946830749512\n",
      "Epoch: 0, Iteration: 11000, Loss: 5.830855369567871\n",
      "Epoch: 0, Iteration: 12000, Loss: 5.565224647521973\n",
      "Epoch: 1, Iteration: 0, Loss: 5.452528476715088\n",
      "Epoch: 1, Iteration: 1000, Loss: 5.588979244232178\n",
      "Epoch: 1, Iteration: 2000, Loss: 5.4638991355896\n",
      "Epoch: 1, Iteration: 3000, Loss: 5.712334156036377\n",
      "Epoch: 1, Iteration: 4000, Loss: 5.5566277503967285\n",
      "Epoch: 1, Iteration: 5000, Loss: 5.5358991622924805\n",
      "Epoch: 1, Iteration: 6000, Loss: 5.27247953414917\n",
      "Epoch: 1, Iteration: 7000, Loss: 5.418829917907715\n",
      "Epoch: 1, Iteration: 8000, Loss: 5.442928314208984\n",
      "Epoch: 1, Iteration: 9000, Loss: 5.209629058837891\n",
      "Epoch: 1, Iteration: 10000, Loss: 5.268213748931885\n",
      "Epoch: 1, Iteration: 11000, Loss: 5.516221046447754\n",
      "Epoch: 1, Iteration: 12000, Loss: 5.031191825866699\n",
      "Epoch: 2, Iteration: 0, Loss: 5.074532508850098\n",
      "Epoch: 2, Iteration: 1000, Loss: 5.156563758850098\n",
      "Epoch: 2, Iteration: 2000, Loss: 5.079566478729248\n",
      "Epoch: 2, Iteration: 3000, Loss: 4.7992262840271\n",
      "Epoch: 2, Iteration: 4000, Loss: 5.0447893142700195\n",
      "Epoch: 2, Iteration: 5000, Loss: 5.128195762634277\n",
      "Epoch: 2, Iteration: 6000, Loss: 4.788591384887695\n",
      "Epoch: 2, Iteration: 7000, Loss: 5.1454291343688965\n",
      "Epoch: 2, Iteration: 8000, Loss: 5.041576862335205\n",
      "Epoch: 2, Iteration: 9000, Loss: 4.995192527770996\n",
      "Epoch: 2, Iteration: 10000, Loss: 5.174893379211426\n",
      "Epoch: 2, Iteration: 11000, Loss: 5.101002216339111\n",
      "Epoch: 2, Iteration: 12000, Loss: 4.998212814331055\n",
      "Epoch: 3, Iteration: 0, Loss: 4.878691673278809\n",
      "Epoch: 3, Iteration: 1000, Loss: 4.824066162109375\n",
      "Epoch: 3, Iteration: 2000, Loss: 4.957627773284912\n",
      "Epoch: 3, Iteration: 3000, Loss: 4.755385875701904\n",
      "Epoch: 3, Iteration: 4000, Loss: 4.855319023132324\n",
      "Epoch: 3, Iteration: 5000, Loss: 4.577142238616943\n",
      "Epoch: 3, Iteration: 6000, Loss: 4.836203098297119\n",
      "Epoch: 3, Iteration: 7000, Loss: 4.912719249725342\n",
      "Epoch: 3, Iteration: 8000, Loss: 4.700368881225586\n",
      "Epoch: 3, Iteration: 9000, Loss: 4.450099945068359\n",
      "Epoch: 3, Iteration: 10000, Loss: 4.779504776000977\n",
      "Epoch: 3, Iteration: 11000, Loss: 4.59440279006958\n",
      "Epoch: 3, Iteration: 12000, Loss: 4.81656551361084\n",
      "Epoch: 4, Iteration: 0, Loss: 4.691129684448242\n",
      "Epoch: 4, Iteration: 1000, Loss: 4.303941249847412\n",
      "Epoch: 4, Iteration: 2000, Loss: 4.616669654846191\n",
      "Epoch: 4, Iteration: 3000, Loss: 4.777889728546143\n",
      "Epoch: 4, Iteration: 4000, Loss: 4.5544843673706055\n",
      "Epoch: 4, Iteration: 5000, Loss: 4.476335048675537\n",
      "Epoch: 4, Iteration: 6000, Loss: 4.725970268249512\n",
      "Epoch: 4, Iteration: 7000, Loss: 4.863439083099365\n",
      "Epoch: 4, Iteration: 8000, Loss: 4.526219844818115\n",
      "Epoch: 4, Iteration: 9000, Loss: 4.35918664932251\n",
      "Epoch: 4, Iteration: 10000, Loss: 4.518848419189453\n",
      "Epoch: 4, Iteration: 11000, Loss: 4.881668567657471\n",
      "Epoch: 4, Iteration: 12000, Loss: 4.59614372253418\n",
      "Epoch: 5, Iteration: 0, Loss: 4.530385971069336\n",
      "Epoch: 5, Iteration: 1000, Loss: 4.3017168045043945\n",
      "Epoch: 5, Iteration: 2000, Loss: 4.627653121948242\n",
      "Epoch: 5, Iteration: 3000, Loss: 4.302238941192627\n",
      "Epoch: 5, Iteration: 4000, Loss: 4.248066425323486\n",
      "Epoch: 5, Iteration: 5000, Loss: 4.551925182342529\n",
      "Epoch: 5, Iteration: 6000, Loss: 4.529759883880615\n",
      "Epoch: 5, Iteration: 7000, Loss: 4.689634323120117\n",
      "Epoch: 5, Iteration: 8000, Loss: 4.359631538391113\n",
      "Epoch: 5, Iteration: 9000, Loss: 4.60862922668457\n",
      "Epoch: 5, Iteration: 10000, Loss: 4.420344829559326\n",
      "Epoch: 5, Iteration: 11000, Loss: 4.350983619689941\n",
      "Epoch: 5, Iteration: 12000, Loss: 4.514793395996094\n",
      "Epoch: 6, Iteration: 0, Loss: 4.641326904296875\n",
      "Epoch: 6, Iteration: 1000, Loss: 4.3040947914123535\n",
      "Epoch: 6, Iteration: 2000, Loss: 4.291695594787598\n",
      "Epoch: 6, Iteration: 3000, Loss: 4.3836283683776855\n",
      "Epoch: 6, Iteration: 4000, Loss: 4.331788063049316\n",
      "Epoch: 6, Iteration: 5000, Loss: 4.354344844818115\n",
      "Epoch: 6, Iteration: 6000, Loss: 4.150676727294922\n",
      "Epoch: 6, Iteration: 7000, Loss: 4.423433780670166\n",
      "Epoch: 6, Iteration: 8000, Loss: 4.323750972747803\n",
      "Epoch: 6, Iteration: 9000, Loss: 4.483421325683594\n",
      "Epoch: 6, Iteration: 10000, Loss: 4.108644962310791\n",
      "Epoch: 6, Iteration: 11000, Loss: 4.386840343475342\n",
      "Epoch: 6, Iteration: 12000, Loss: 4.246557712554932\n",
      "Epoch: 7, Iteration: 0, Loss: 4.2951979637146\n",
      "Epoch: 7, Iteration: 1000, Loss: 4.293236255645752\n",
      "Epoch: 7, Iteration: 2000, Loss: 4.14286470413208\n",
      "Epoch: 7, Iteration: 3000, Loss: 4.245637893676758\n",
      "Epoch: 7, Iteration: 4000, Loss: 4.282914638519287\n",
      "Epoch: 7, Iteration: 5000, Loss: 4.418691158294678\n",
      "Epoch: 7, Iteration: 6000, Loss: 4.273174285888672\n",
      "Epoch: 7, Iteration: 7000, Loss: 4.2630791664123535\n",
      "Epoch: 7, Iteration: 8000, Loss: 4.212876319885254\n",
      "Epoch: 7, Iteration: 9000, Loss: 4.125677585601807\n",
      "Epoch: 7, Iteration: 10000, Loss: 4.271975517272949\n",
      "Epoch: 7, Iteration: 11000, Loss: 3.982935667037964\n",
      "Epoch: 7, Iteration: 12000, Loss: 4.307538032531738\n",
      "Epoch: 8, Iteration: 0, Loss: 4.235093593597412\n",
      "Epoch: 8, Iteration: 1000, Loss: 4.236294746398926\n",
      "Epoch: 8, Iteration: 2000, Loss: 4.379791259765625\n",
      "Epoch: 8, Iteration: 3000, Loss: 4.046074867248535\n",
      "Epoch: 8, Iteration: 4000, Loss: 4.2508864402771\n",
      "Epoch: 8, Iteration: 5000, Loss: 4.213134288787842\n",
      "Epoch: 8, Iteration: 6000, Loss: 4.153458118438721\n",
      "Epoch: 8, Iteration: 7000, Loss: 4.100075721740723\n",
      "Epoch: 8, Iteration: 8000, Loss: 4.1623125076293945\n",
      "Epoch: 8, Iteration: 9000, Loss: 4.233426570892334\n",
      "Epoch: 8, Iteration: 10000, Loss: 4.3431477546691895\n",
      "Epoch: 8, Iteration: 11000, Loss: 4.4455413818359375\n",
      "Epoch: 8, Iteration: 12000, Loss: 4.353407382965088\n",
      "Epoch: 9, Iteration: 0, Loss: 4.3775954246521\n",
      "Epoch: 9, Iteration: 1000, Loss: 4.284245491027832\n",
      "Epoch: 9, Iteration: 2000, Loss: 4.348446846008301\n",
      "Epoch: 9, Iteration: 3000, Loss: 4.479431629180908\n",
      "Epoch: 9, Iteration: 4000, Loss: 4.109002590179443\n",
      "Epoch: 9, Iteration: 5000, Loss: 4.095792770385742\n",
      "Epoch: 9, Iteration: 6000, Loss: 4.217097282409668\n",
      "Epoch: 9, Iteration: 7000, Loss: 4.403225898742676\n",
      "Epoch: 9, Iteration: 8000, Loss: 4.232264518737793\n",
      "Epoch: 9, Iteration: 9000, Loss: 4.336725234985352\n",
      "Epoch: 9, Iteration: 10000, Loss: 4.205324649810791\n",
      "Epoch: 9, Iteration: 11000, Loss: 4.388310432434082\n",
      "Epoch: 9, Iteration: 12000, Loss: 4.029799938201904\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T19:09:41.016629Z",
     "start_time": "2024-07-26T19:09:40.778542Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.load_state_dict(torch.load('my_model1.pt', map_location=device))\n",
    "\n",
    "print(\"Model loaded successfully.\")\n",
    "print(model)\n",
    "print(f\"number of parameters: {model.get_num_params()/1e6:.2f}M\")\n",
    "# Load the tokenizer"
   ],
   "id": "61b0078f77ee3622",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(25000, 128)\n",
      "    (wpe): Embedding(128, 128)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-5): 6 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=128, out_features=384, bias=True)\n",
      "          (c_proj): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=128, out_features=25000, bias=False)\n",
      ")\n",
      "number of parameters: 4.39M\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T19:14:04.862723Z",
     "start_time": "2024-07-26T19:14:04.834568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "def tokenize_and_truncate_text(text, block_size):\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
    "    if len(tokens) > block_size:\n",
    "        tokens = tokens[-block_size:]  \n",
    "    return torch.tensor(tokens, dtype=torch.long).unsqueeze(0)  \n",
    "\n",
    "def decode_output(predicted_ids):\n",
    "    predicted_tokens = predicted_ids.tolist()  \n",
    "    predicted_text = tokenizer.decode(predicted_tokens, skip_special_tokens=False)\n",
    "    return predicted_text\n",
    "\n",
    "\n",
    "def get_model_output(model, input_text, block_size, temperature=1.0, top_k=50):\n",
    "    model.eval()\n",
    "\n",
    "    input_ids = tokenize_and_truncate_text(input_text, block_size)\n",
    "    input_ids = input_ids.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits, _ = model(input_ids)\n",
    "        logits = logits[:, -1, :] / temperature  \n",
    "\n",
    "       \n",
    "        top_k_logits, top_k_indices = torch.topk(logits, k=top_k, dim=-1)\n",
    "        probs = F.softmax(top_k_logits, dim=-1)\n",
    "\n",
    "        \n",
    "        chosen_idx = torch.multinomial(probs, num_samples=1)\n",
    "        predicted_token_id = top_k_indices[0, chosen_idx[0]]\n",
    "\n",
    "       \n",
    "        predicted_text = decode_output(predicted_token_id)\n",
    "\n",
    "    return predicted_text\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def poem(input_text,beam=15):\n",
    "    complete_text = input_text\n",
    "    last_few_tokens = []\n",
    "    temperature = 0.8\n",
    "    bom_count = 0\n",
    "\n",
    "    while bom_count < beam:\n",
    "        predicted_text = get_model_output(model, input_text, config.block_size, temperature=temperature, top_k=50)\n",
    "\n",
    "        if predicted_text.strip() in last_few_tokens :\n",
    "            temperature *= 1.1  \n",
    "            predicted_text = get_model_output(model, input_text, config.block_size, temperature=temperature, top_k=50)\n",
    "        else:\n",
    "            temperature = max(0.7, temperature * 0.95) \n",
    "\n",
    "        complete_text += \" \" + predicted_text.strip()  \n",
    "        last_few_tokens.append(predicted_text.strip())\n",
    "\n",
    "        if len(last_few_tokens) > 3:  \n",
    "            last_few_tokens.pop(0)\n",
    "\n",
    "        input_text += \" \" + predicted_text.strip()\n",
    "        input_ids = tokenize_and_truncate_text(input_text, config.block_size)\n",
    "        input_text = tokenizer.decode(input_ids[0].tolist())  \n",
    "\n",
    "        if '[EOS]' in predicted_text or '[BOM]' in predicted_text:\n",
    "            complete_text += '\\n'\n",
    "            bom_count += 1\n",
    "\n",
    "    lines = complete_text.split('\\n')\n",
    "    for line in lines:\n",
    "        print(line)\n",
    "    print('-' * 100)\n"
   ],
   "id": "9d5cdbf7fe9c8336",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-26T19:21:23.785697Z",
     "start_time": "2024-07-26T19:21:22.649216Z"
    }
   },
   "cell_type": "code",
   "source": "poem(input_text='دوستت دارم')",
   "id": "c5e8d8cc9bd068f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "دوستت دارم که سد مرغ او را [EOS]\n",
      " [BOM]\n",
      " چو بر سر در او با من و هم زبان باشد [EOS]\n",
      " [BOM]\n",
      " به جان  باشد و نمی باشد [EOS]\n",
      " [BOM]\n",
      " که می آید به جای خویش را چه باشد [EOS]\n",
      " [BOM]\n",
      " به یک دم از دست و می باشد [EOS]\n",
      " [BOM]\n",
      " اگر یار و نه من نکرد آن باشد [EOS]\n",
      " [BOM]\n",
      " که از آن خانه من باشد [EOS]\n",
      " [BOM]\n",
      " که چون باشد و از پی بی خان و جانی [EOS]\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "782d74296035e7e6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
